{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from utils.pipeline import Context, Step\n",
    "from utils.pipeline_lib import *\n",
    "from tqdm import tqdm\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "\n",
    "with open(\"data/geoapify.key\", \"r\") as  f:\n",
    "    geoapify_api_key = f.readline().rstrip(\"\\n\").strip()\n",
    "# generated file paths\n",
    "saved_context_ct_path = \"data/h1n1/h1n1_ctx\"\n",
    "export_seq_id_path = \"data/h1n1/h1n1_08-09_dataset.csv\"\n",
    "db_url = 'sqlite:///data/h1n1/h1n1.sqlite'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RSCU, dinucleotides and other attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = Context()\n",
    "\n",
    "# DATA\n",
    "class Data(Step):\n",
    "    def run(self):\n",
    "        return {\n",
    "            'output': pd.read_csv(...),         # replace with your data\n",
    "            'metadata_feature_names': [...],    # the list of metadata column names from your file\n",
    "            'data_feature_names': [\"CDS\"]       \n",
    "        }\n",
    "S_global_data = Data()\n",
    "S_global_data_1 = Input.AssignHostType(depends_on=S_global_data)\n",
    "S_global_data_2 = Input.RSCU(depends_on=S_global_data_1, ctx=ct)\n",
    "S_transformation_1 = DataTransform.LogBySynCount(depends_on=S_global_data_2)\n",
    "S_transformation_2 = DataTransform.PlainAndLogDinucleotide(depends_on=S_transformation_1, ctx=ct)\n",
    "S_partition_data = WindowSelector(date_range=DateRange.from_iso_weeks('2008-01', '2010-10'), start=\"2008-01\", end=\"2010-10\", depends_on=S_transformation_2, name_alias=\"NarrowGlobalData\", ctx=ct)\n",
    "S_ordered_data = AbsoluteSortByCollectionDateAndIndex(depends_on=S_partition_data, ctx=ct)\n",
    "\n",
    "print(\"!! Using filter boundary box of North America\")\n",
    "# start geolocate cache\n",
    "geolocate_cache_path = \"data/cache/cache.parquet\"\n",
    "geolocate_cache = pd.read_parquet(geolocate_cache_path) if os.path.exists(geolocate_cache_path) else Geolocation.GeoapifyBatchJobRequest.get_new_cache()\n",
    "\n",
    "# geolocate\n",
    "S_geolocate_1 = Geolocation.GeoapifyBatchJobRequest(geoapify_api_key, continent_bounding_box=Geoapify.continent_bounding_box['North America'], \n",
    "                                                    cache=geolocate_cache, \n",
    "                                                    depends_on=S_ordered_data, ctx=ct)\n",
    "geolocate_cache = S_geolocate_1.cache\n",
    "S_geolocate_2 = Geolocation.GeoapifyParseBatchRequestOutput(geoapify_api_key, cache=geolocate_cache, \n",
    "                                                            depends_on=[S_geolocate_1, S_ordered_data])\n",
    "geolocate_cache = S_geolocate_2.cache\n",
    "\n",
    "S_geolocate_3 = Geolocation.GuessCountryOrStateFromSequenceName(geoapify_api_key, continent_bounding_box=Geoapify.continent_bounding_box['North America'], \n",
    "                                                                cache=geolocate_cache, \n",
    "                                                                depends_on=S_geolocate_2, ctx=ct)\n",
    "geolocate_cache = S_geolocate_3.cache\n",
    "\n",
    "S_global_data_sorted_bmc_annotated_1 = InputH1N1.AttachBMC_GenomicsCluster(depends_on=S_geolocate_3)\n",
    "S_global_data_sorted_bmc_annotated_2 = AttachWeek(depends_on=S_global_data_sorted_bmc_annotated_1, ctx=ct)\n",
    "\n",
    "result_input_data = S_global_data_sorted_bmc_annotated_2.materialize()\n",
    "\n",
    "# del ct['GeoapifyBatchJobRequest']\n",
    "ct.store(saved_context_ct_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### write table of input sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cnx.dispose()\n",
    "cnx = create_engine(db_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "write result_input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "with cnx.connect() as connection:\n",
    "    (result_input_data.output[['Lineage', 'Clade', 'Location', 'Host', 'Host_Type', 'Collection_Date', 'Submission_Date', 'Sort_Key', 'lat', 'lon', 'country_or_state', 'country', 'state', 'bmc_cluster_label', 'Week']]\n",
    "     .to_sql(name='input_data', con=connection, if_exists='replace'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "compute and write warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(S_data, stray_window_size, stray_k):\n",
    "    ct = Context.load(saved_context_ct_path)\n",
    "    S_partitioner = Stray.MovingWindowFixedSize(evaluation_date_range=DateRange.from_iso_weeks('2008-31', '2010-01'), \n",
    "                                                train_size=stray_window_size-1, test_size=1, window_shift=1,\n",
    "                                                depends_on=S_data, ctx=ct)\n",
    "    # GET WINDOW NAMES\n",
    "    result_partitioner = S_partitioner.materialize()\n",
    "    train_plus_eval_index_range = result_partitioner.train_plus_test_range_names.values()\n",
    "    eval_index_range = result_partitioner.test_range_names.values()\n",
    "\n",
    "    # GET PARTITIONS\n",
    "    S_train_plus_test_partitions = [NumericIndexBasedPartition(partitioner_function_name='.next_train_plus_test_partition', \n",
    "                                                            range_start=s, range_end=e,\n",
    "                                                            depends_on=S_partitioner, ctx=ct,\n",
    "                                                            name_alias=f\"TrainEvalPartition_{s}_{e}\") for s,e in train_plus_eval_index_range]\n",
    "    S_eval_partitions = [NumericIndexBasedPartition(partitioner_function_name='.next_test_partition', \n",
    "                                                    range_start=s, range_end=e,\n",
    "                                                    depends_on=S_partitioner, ctx=ct,\n",
    "                                                    name_alias=f\"EvalPartition_{s}_{e}\") for s,e in eval_index_range]\n",
    "    \n",
    "    # STRAY\n",
    "    S_outlier_detection = [Stray.OutlierDetection(k=stray_k, depends_on=x, name_alias=f\"OutlierDetection{s}_{e}\") for x,(s,e) in zip(S_train_plus_test_partitions,eval_index_range)]\n",
    "    S_train_plus_eval_partitions_enriched = [Stray.MapOutliersToOriginalData(depends_on=[s1,s2], \n",
    "                                                                            name_alias=f\"MapOutliersToOriginalData_{s}_{e}\") for s1,s2,(s,e) in zip(S_train_plus_test_partitions,S_outlier_detection,eval_index_range)]\n",
    "    S_eval_partitions_enriched = [IndexBasedFilter(depends_on=[i1,i2], name_alias=f\"IndexBasedFilter_{s}_{e}\") for i1,i2,(s,e) in zip(S_train_plus_eval_partitions_enriched, S_eval_partitions, eval_index_range)]\n",
    "    S_eval_outliers_enriched = [Stray.FilterOutliers(depends_on=x, name_alias=f\"FilterOutliers_{s}_{e}\") for x,(s,e) in zip(S_eval_partitions_enriched, eval_index_range)]\n",
    "\n",
    "    # COLLECTIVE EVALUATION\n",
    "    S_all_outliers = Stray.CollectOutlierIdMultipleWindows(depends_on=S_eval_outliers_enriched)\n",
    "    S_tested_data = WindowSelector(date_range=DateRange.from_iso_weeks('2008-31', '2010-01'),start=\"2008-31\",end=\"2010-01\", inclusive=\"left\", depends_on=S_data, name_alias=\"EvalData\")\n",
    "    S_global_data_annotated_1 = Stray.AnnotateOutliersinOriginalData(depends_on=[S_tested_data, S_all_outliers])\n",
    "\n",
    "    S_outliers_count = Stray.CollectOutlierCountMultipleWindows(depends_on=S_global_data_annotated_1)\n",
    "    S_global_data_annotated_3 = Stray.AnnotateOutliersCountInOriginalData(depends_on=[S_global_data_annotated_1, S_outliers_count])\n",
    "\n",
    "    # ##########  RUN\n",
    "    head = S_global_data_annotated_3\n",
    "    result = head.materialize()\n",
    "    return result\n",
    "\n",
    "def format_detail_table(result: ResultType[Stray.AnnotateOutliersCountInOriginalData]):\n",
    "    # outlier_count = result.output.outlier.sum()\n",
    "    a = result.output[result.output.outlier].copy()\n",
    "    a['Week'] = a.Collection_Date.dt.strftime(\"%G-%V\")\n",
    "    a = a[['Lineage', 'Clade', 'Location', 'Host', 'Host_Type', 'Collection_Date', 'Submission_Date', 'Flu_Season', 'Sort_Key', 'lat', 'lon', 'country_or_state', 'country', 'state', 'bmc_cluster_label', 'Week']]\n",
    "    return a \n",
    "\n",
    "def write_sqlite_table(df, name, cnx):\n",
    "    with cnx.connect() as connection:\n",
    "        df.to_sql(name=name, con=connection, if_exists='replace')\n",
    "\n",
    "def loop(window_sizes, ks):\n",
    "    progress = tqdm(total=len(window_sizes)*len(ks))\n",
    "    comb_n = 0\n",
    "    \n",
    "    for window_size in tqdm(window_sizes):\n",
    "        for k in tqdm(ks):\n",
    "            if k >= window_size:\n",
    "                continue\n",
    "\n",
    "            \n",
    "            try:\n",
    "                result_unformatted = run(S_global_data_sorted_bmc_annotated_2, window_size, k)\n",
    "                ## tabella di sequenze con almeno un warning \n",
    "            except Exception as e:\n",
    "                print(f\"Error while testing combination (comb_n) {comb_n}: {window_size} {k}\")\n",
    "                raise e\n",
    "                      \n",
    "            try:\n",
    "                detail_table = format_detail_table(result_unformatted)\n",
    "            except Exception as e:\n",
    "                print(f\"Error while formatting combination (comb_n) {comb_n}: {window_size} {k}\")\n",
    "                raise e\n",
    "            \n",
    "            try:\n",
    "                write_sqlite_table(detail_table, name=f\"window{window_size}_k{k}\", cnx=cnx)\n",
    "            except Exception as e:\n",
    "                print(f\"Error while writing warnings detail file of combination (comb_n) {comb_n}: {window_size} {k}\")\n",
    "                raise e\n",
    "            \n",
    "            comb_n += 1\n",
    "            progress.update(1)\n",
    "                               \n",
    "loop(window_sizes=(5,10,50,100), ks=(1,3,5,10,15))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
