{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from utils.pipeline import Context, Step\n",
    "from utils.pipeline_lib import *\n",
    "from tqdm import tqdm\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# generated file paths\n",
    "saved_context_ct_path = \"data/h5n1/h1n1_ctx\"\n",
    "export_seq_id_path = \"data/h5n1/h1n1_08-09_dataset.csv\"\n",
    "db_url = 'sqlite:///data/h5n1/h1n1.sqlite'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RSCU, dinucleotides and other attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = Context()\n",
    "\n",
    "# DATA\n",
    "class Data(Step):\n",
    "    def run(self):\n",
    "        return {\n",
    "            'output': pd.read_csv(...),         # replace with your data\n",
    "            'metadata_feature_names': [...],    # the list of metadata column names from your file\n",
    "            'data_feature_names': [\"CDS\"]       \n",
    "        }\n",
    "S_global_data = Data()\n",
    "S_global_data_1 = Input.AssignHostType(depends_on=S_global_data)\n",
    "S_global_data_2 = Input.RSCU(depends_on=S_global_data_1, ctx=ct)\n",
    "S_transformation_1 = DataTransform.LogBySynCount(depends_on=S_global_data_2)\n",
    "S_transformation_2 = DataTransform.PlainAndLogDinucleotide(depends_on=S_transformation_1, ctx=ct)\n",
    "S_ordered_data = AbsoluteSortByCollectionDateAndIndex(depends_on=S_transformation_2, ctx=ct)\n",
    "S_global_data_sorted_bmc_annotated_1 = InputH5N1.AttachBMC_GenomicsCluster(depends_on=S_ordered_data)\n",
    "S_global_data_sorted_bmc_annotated_2 = AttachWeek(depends_on=S_global_data_sorted_bmc_annotated_1, ctx=ct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_input_data = S_global_data_sorted_bmc_annotated_2.materialize()\n",
    "print(result_input_data.output.shape)\n",
    "# Assert the DF is still sorted by Sort_Key\n",
    "pd.testing.assert_series_equal(result_input_data.output.Sort_Key, result_input_data.output.Sort_Key.sort_values())\n",
    "\" \".join(result_input_data.output.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "exported_columns = ['Lineage', 'Clade', 'Location', 'Host', 'Collection_Date', 'Submission_Date', 'Host_Type', 'Sort_Key',  \n",
    "                               'bmc_cluster_label', 'Week', \n",
    "                               'Pathogenicity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct.store(saved_context_ct_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### write table of input sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cnx.dispose()\n",
    "cnx = create_engine(db_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "write result_input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with cnx.connect() as connection:\n",
    "    (result_input_data.output[exported_columns]\n",
    "     .to_sql(name='input_data', con=connection, if_exists='replace'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "compute and write warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_windows(S_data, stray_window_size):\n",
    "    ct = Context.load(saved_context_ct_path)\n",
    "    S_partitioner = Stray.MovingWindowFixedSize(evaluation_date_range=DateRange.from_iso_weeks('2019-36', '2025-04'), \n",
    "                                                train_size=stray_window_size-1, test_size=1, window_shift=1,\n",
    "                                                depends_on=S_data, ctx=ct)\n",
    "    # GET WINDOW NAMES\n",
    "    result_partitioner = S_partitioner.materialize()\n",
    "    train_plus_eval_index_range = result_partitioner.train_plus_test_range_names.values()\n",
    "    eval_index_range = result_partitioner.test_range_names.values()\n",
    "\n",
    "    # GET PARTITIONS\n",
    "    S_train_plus_test_partitions = [NumericIndexBasedPartition(partitioner_function_name='.next_train_plus_test_partition', \n",
    "                                                            range_start=s, range_end=e,\n",
    "                                                            depends_on=S_partitioner, ctx=ct,\n",
    "                                                            name_alias=f\"TrainEvalPartition_{s}_{e}\") for s,e in train_plus_eval_index_range]\n",
    "    S_eval_partitions = [NumericIndexBasedPartition(partitioner_function_name='.next_test_partition', \n",
    "                                                    range_start=s, range_end=e,\n",
    "                                                    depends_on=S_partitioner, ctx=ct,\n",
    "                                                    name_alias=f\"EvalPartition_{s}_{e}\") for s,e in eval_index_range]\n",
    "    return ct, S_partitioner, result_partitioner, train_plus_eval_index_range, eval_index_range, S_train_plus_test_partitions, S_eval_partitions\n",
    "    \n",
    "def run_stray4k(S_data, ct, S_partitioner, result_partitioner, train_plus_eval_index_range, eval_index_range, S_train_plus_test_partitions, S_eval_partitions, stray_k):\n",
    "    # STRAY\n",
    "    S_outlier_detection = [Stray.OutlierDetection(k=stray_k, depends_on=x, name_alias=f\"OutlierDetection{s}_{e}\") for x,(s,e) in zip(S_train_plus_test_partitions,eval_index_range)]\n",
    "    S_train_plus_eval_partitions_enriched = [Stray.MapOutliersToOriginalData(depends_on=[s1,s2], \n",
    "                                                                            name_alias=f\"MapOutliersToOriginalData_{s}_{e}\") for s1,s2,(s,e) in zip(S_train_plus_test_partitions,S_outlier_detection,eval_index_range)]\n",
    "    S_eval_partitions_enriched = [IndexBasedFilter(depends_on=[i1,i2], name_alias=f\"IndexBasedFilter_{s}_{e}\") for i1,i2,(s,e) in zip(S_train_plus_eval_partitions_enriched, S_eval_partitions, eval_index_range)]\n",
    "    S_eval_outliers_enriched = [Stray.FilterOutliers(depends_on=x, name_alias=f\"FilterOutliers_{s}_{e}\") for x,(s,e) in zip(S_eval_partitions_enriched, eval_index_range)]\n",
    "\n",
    "    # COLLECTIVE EVALUATION\n",
    "    S_all_outliers = Stray.CollectOutlierIdMultipleWindows(depends_on=S_eval_outliers_enriched)\n",
    "    S_tested_data = WindowSelector(date_range=DateRange.from_iso_weeks('2019-36', '2025-04'),start=\"2019-36\",end=\"2025-04\", inclusive=\"left\", depends_on=S_data, name_alias=\"EvalData\")\n",
    "    S_global_data_annotated_1 = Stray.AnnotateOutliersinOriginalData(depends_on=[S_tested_data, S_all_outliers])\n",
    "\n",
    "    S_outliers_count = Stray.CollectOutlierCountMultipleWindows(depends_on=S_global_data_annotated_1)\n",
    "    S_global_data_annotated_3 = Stray.AnnotateOutliersCountInOriginalData(depends_on=[S_global_data_annotated_1, S_outliers_count])\n",
    "\n",
    "    # ##########  RUN\n",
    "    head = S_global_data_annotated_3\n",
    "    result = head.materialize()\n",
    "    return result\n",
    "\n",
    "def format_detail_table(result: ResultType[Stray.AnnotateOutliersCountInOriginalData]):\n",
    "    # outlier_count = result.output.outlier.sum()\n",
    "    a = result.output[result.output.outlier].copy()\n",
    "    a['Week'] = a.Collection_Date.dt.strftime(\"%G-%V\")\n",
    "    a = a[exported_columns]\n",
    "    return a \n",
    "\n",
    "def write_sqlite_table(df, name, cnx):\n",
    "    with cnx.connect() as connection:\n",
    "        df.to_sql(name=name, con=connection, if_exists='replace')\n",
    "\n",
    "def loop(window_sizes, ks):\n",
    "    progress = tqdm(total=len(window_sizes)*len(ks))\n",
    "    comb_n = 0\n",
    "    \n",
    "    for window_size in tqdm(window_sizes):\n",
    "        args = run_windows(S_global_data_sorted_bmc_annotated_2, window_size)\n",
    "        for k in tqdm(ks):\n",
    "            if k >= window_size:\n",
    "                continue\n",
    "           \n",
    "            try:\n",
    "                result_unformatted = run_stray4k(S_global_data_sorted_bmc_annotated_2, *args, k)\n",
    "                ## tabella di sequenze con almeno un warning \n",
    "            except Exception as e:\n",
    "                print(f\"Error while testing combination (comb_n) {comb_n}: {window_size} {k}\")\n",
    "                raise e\n",
    "            \n",
    "            try:\n",
    "                detail_table = format_detail_table(result_unformatted)\n",
    "            except Exception as e:\n",
    "                print(f\"Error while formatting combination (comb_n) {comb_n}: {window_size} {k}\")\n",
    "                raise e\n",
    "            \n",
    "            try:\n",
    "                write_sqlite_table(detail_table, name=f\"window{window_size}_k{k}\", cnx=cnx)\n",
    "            except Exception as e:\n",
    "                print(f\"Error while writing warnings detail file of combination (comb_n) {comb_n}: {window_size} {k}\")\n",
    "                raise e\n",
    "            \n",
    "            comb_n += 1\n",
    "        \n",
    "            progress.update(1)\n",
    "                               \n",
    "print(f\"{4 * 5} input combinations\")\n",
    "loop(window_sizes=(5,10,50,100), ks=(1,3,5,10,15))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
