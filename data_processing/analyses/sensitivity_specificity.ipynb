{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from utils.pipeline import Context, Step\n",
    "from utils.pipeline_lib import *\n",
    "from tqdm import tqdm\n",
    "from sqlalchemy import create_engine\n",
    "from math import floor\n",
    "\n",
    "# generated file paths\n",
    "saved_context_ct_path = \"data/ss/ss_v10_Ctx\"\n",
    "export_seq_id_path = \"data/ss/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RSCU, dinucleotides and other attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = Context()\n",
    "ct_path = saved_context_ct_path\n",
    "\n",
    "# DATA\n",
    "class Data(Step):\n",
    "    def run(self):\n",
    "        return {\n",
    "            'output': pd.read_csv(...),         # replace with your data\n",
    "            'metadata_feature_names': [...],    # the list of metadata column names from your file\n",
    "            'data_feature_names': [\"CDS\"]       \n",
    "        }\n",
    "S_global_data = Data(name_alias=\"H1N1\")\n",
    "S_global_data_1 = Input.AssignHostType(depends_on=S_global_data)\n",
    "S_global_data_2 = Input.RSCU(depends_on=S_global_data_1, ctx=ct)\n",
    "S_transformation_1 = DataTransform.LogBySynCount(depends_on=S_global_data)\n",
    "S_transformation_2 = DataTransform.PlainAndLogDinucleotide(depends_on=S_transformation_1, ctx=ct)\n",
    "\n",
    "S_partition_data = WindowSelector(date_range=DateRange.from_iso_weeks('2008-01', '2010-10'), start=\"2008-01\", end=\"2010-10\", depends_on=S_transformation_2, name_alias=\"NarrowGlobalData\", ctx=ct)\n",
    "S_ordered_data = AbsoluteSortByCollectionDateAndIndex(depends_on=S_partition_data, ctx=ct)\n",
    "S_global_geolocated_sorted_bmc_annotated = InputH1N1.AttachBMC_GenomicsCluster(depends_on=S_ordered_data)\n",
    "result_input_data = S_global_geolocated_sorted_bmc_annotated.materialize()\n",
    "# Add Week name\n",
    "result_input_data.output['Week'] = result_input_data.output['Collection_Date'].dt.strftime(\"%G-%V\")\n",
    "\n",
    "# Assert the DF is still sorted by Sort_Key\n",
    "pd.testing.assert_series_equal(result_input_data.output.Sort_Key, result_input_data.output.Sort_Key.sort_values())\n",
    "\n",
    "# del ct['GeoapifyBatchJobRequest']\n",
    "ct.store(ct_path)\n",
    "print(result_input_data.output.shape)\n",
    "print(S_ordered_data.materialize().output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bmc_cluster_label\n",
       "0                    2642\n",
       "1                     663\n",
       "3                      68\n",
       "4                       3\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_input_data.output[['bmc_cluster_label']].value_counts(dropna=False).sort_index(level=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "swine_df = result_input_data.output[result_input_data.output.Host_Type == \"swine\"]\n",
    "human_df = result_input_data.output[result_input_data.output.Host_Type == \"human\"]\n",
    "\n",
    "# final partitions\n",
    "human_bmc_cluster_1 = human_df[(human_df.bmc_cluster_label == 1)]\n",
    "human_bmc_cluster_0 = human_df[(human_df.bmc_cluster_label == 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal sequences 168\n",
      "Outlier sequences 100\n",
      "Total 268\n"
     ]
    }
   ],
   "source": [
    "# select time period, sorting and renaming\n",
    "normal_df = human_bmc_cluster_1[\n",
    "        (human_bmc_cluster_1.Collection_Date >= pd.to_datetime(\"2009-01-25\"))          # first sequences in late april-may\n",
    "        & (human_bmc_cluster_1.Collection_Date <= pd.to_datetime(\"2009-03-14\"))\n",
    "    ].sort_values(by=\"Collection_Date\", ascending=True)\n",
    "outlier_df = human_bmc_cluster_0[\n",
    "        (human_bmc_cluster_0.Collection_Date >= pd.to_datetime(\"2008-07-28\")) \n",
    "        & (human_bmc_cluster_0.Collection_Date <= pd.to_datetime(\"2010-01-10\"))        # last sequences in studied period\n",
    "        & ~(pd.isna(human_bmc_cluster_0.Lineage))\n",
    "    ].sort_values(by=\"Collection_Date\", ascending=True).iloc[-100:,:]\n",
    "\n",
    "print(\"Normal sequences\", normal_df.shape[0])\n",
    "print(\"Outlier sequences\", outlier_df.shape[0])\n",
    "print(\"Total\", normal_df.shape[0] + outlier_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_df['true_outlier'] = False\n",
    "outlier_df['true_outlier'] = True\n",
    "final_data = pd.concat([normal_df, outlier_df])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collection Date range for \n",
    "- 100 normal sequences\n",
    "- 100 outlier sequences\n",
    "- additional 30 normal sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection_Date range of 100 G1 normal sequences\n",
      "2009/01/26 2009/02/20 25 days\n",
      "Collection_Date range of 100 G2 outlier sequences\n",
      "2009/12/14 2010/01/10 27 days\n",
      "Collection_Date range of 30 G1 noise/normal sequences\n",
      "2009/02/20 2009/03/01 9 days\n"
     ]
    }
   ],
   "source": [
    "def print_range(dataset_df):\n",
    "    d = pd.to_datetime(dataset_df.Collection_Date.describe()[['min', 'max']])\n",
    "    print(\" \".join(\n",
    "        d.dt.strftime(\"%Y/%m/%d\").tolist() + [str((d['max'] - d['min']).days), \"days\"]\n",
    "    ))\n",
    "print(\"Collection_Date range of 100 G1 normal sequences\")\n",
    "print_range(normal_df.iloc[:100])\n",
    "print(\"Collection_Date range of 100 G2 outlier sequences\")\n",
    "print_range(outlier_df.iloc[:100])\n",
    "print(\"Collection_Date range of 30 G1 noise/normal sequences\")\n",
    "print_range(normal_df.iloc[100:130])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export of sequence IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset_id(df):\n",
    "    return df[['Isolate_Id', 'Isolate_Name']]\n",
    "\n",
    "generate_dataset_id(normal_df.iloc[:100]).to_csv(f\"{export_seq_id_path}/g1_dataset.csv\", index=False)\n",
    "generate_dataset_id(outlier_df.iloc[:100]).to_csv(f\"{export_seq_id_path}/g2_dataset.csv\", index=False)\n",
    "generate_dataset_id(normal_df.iloc[100:130]).to_csv(f\"{export_seq_id_path}/noise_dataset.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper data stucture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    del ct['Data']\n",
    "except KeyError:\n",
    "    pass\n",
    "S_data = Step.stepify({\n",
    "    'output': final_data, \n",
    "    'data_feature_names': result_input_data.data_feature_names, \n",
    "    'metadata_feature_names': result_input_data.metadata_feature_names}\n",
    "    , name_alias=\"Data\", ctx=ct)\n",
    "_ = S_data.materialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_exported_columns = ['Lineage', 'Clade', 'Location', 'Host', 'Collection_Date', 'Submission_Date', 'Host_Type', 'Flu_Season', 'Sort_Key', 'bmc_cluster_label', 'Week', 'true_outlier']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Measure Warnings based on Window, K, Noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Save base context in\", ct_path)\n",
    "ct.store(ct_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StrayMixedNormalOutlierWindows(Step):\n",
    "    \"\"\"\n",
    "    Train and test in this context are interpreted as follows:\n",
    "    Fixed size partitions may need data points that are not included in the given input_data (i.e., data points preceeding the first one). Such partitions are ignored (the output range_names etc. do not contain such partitions).\n",
    "    \"\"\"\n",
    "    def __init__(self, train_size=99, test_size=1, window_shift=1, n_wrong_outliers=0, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.train_size = train_size\n",
    "        self.test_size = test_size\n",
    "        self.window_size = train_size + test_size\n",
    "        self.window_shift = window_shift\n",
    "        self.n_wrong_outlier = n_wrong_outliers\n",
    "\n",
    "    def run(self, input_data: ResultType) -> ResultType:\n",
    "        data = input_data.output\n",
    "        data_len = data.shape[0]\n",
    "        assert not data.empty, \"Input DataFrame is empty. Cannot extract any partition.\"\n",
    "        assert data['true_outlier'].sum() > 0, \"Input DataFrame do not contain any outlier. Cannot extract any partition.\"\n",
    "        assert (~data['true_outlier']).sum() > 0, \"Input DataFrame do not contain any non-outlier data. Cannot extract any partition.\"\n",
    "    \n",
    "        normal_data = data[~data.true_outlier].iloc[:self.window_size,:]\n",
    "        outlier_data = data[data.true_outlier].iloc[-self.window_size:,:,]#.iloc[::-1]\n",
    "        \n",
    "        rand_samples_idx = np.array([])\n",
    "        if self.n_wrong_outlier > 0:\n",
    "            outlier_data = outlier_data.reset_index()\n",
    "            rand_samples_idx  = np.random.choice(int(floor(outlier_data.shape[0])), size=self.n_wrong_outlier, replace=False)\n",
    "            outlier_data = outlier_data.drop(index=rand_samples_idx)\n",
    "            wrong_outliers = data[~data.true_outlier].reset_index().iloc[100:100+self.n_wrong_outlier,:].set_index(pd.Index(rand_samples_idx))\n",
    "            outlier_data = pd.concat([outlier_data, wrong_outliers]).sort_index().set_index(\"index\")\n",
    "            try:\n",
    "                assert (\n",
    "                    outlier_data['true_outlier'].sum() == (self.window_size - self.n_wrong_outlier) \n",
    "                    and (~outlier_data['true_outlier']).sum() == self.n_wrong_outlier \n",
    "                    and outlier_data.shape[0] == self.window_size), \"Contaimation gone wrong\"\n",
    "            except AssertionError:\n",
    "                raise\n",
    "        idx_stray_window_with_noise = 1 + rand_samples_idx\n",
    "        # find position of stray window where quantity >= non-outlier quantity\n",
    "        idx_g1_equal_g2_size = 0\n",
    "        for w_idx in range(self.window_size):\n",
    "            g1_wo_noise_size = self.window_size - w_idx\n",
    "            g1_noise = (idx_stray_window_with_noise <= w_idx).sum()\n",
    "            g1_w_noise_size = g1_wo_noise_size + g1_noise\n",
    "            g2_size = self.window_size - g1_w_noise_size\n",
    "            if g2_size >= g1_w_noise_size:\n",
    "                idx_g1_equal_g2_size = w_idx\n",
    "                break\n",
    "        idx_stray_window_with_noise = np.sort(idx_stray_window_with_noise)\n",
    "        if len(idx_stray_window_with_noise) > 0:\n",
    "            last_idx_stray_window_with_noise = idx_stray_window_with_noise[-1].item()\n",
    "            idx_stray_windows_with_noise_within_half_window = idx_stray_window_with_noise[idx_stray_window_with_noise < idx_g1_equal_g2_size]\n",
    "            idx_stray_windows_with_noise_within_half_window = idx_stray_windows_with_noise_within_half_window[-1].item() if len(idx_stray_windows_with_noise_within_half_window) > 0 else None\n",
    "        else:\n",
    "            last_idx_stray_window_with_noise = None\n",
    "            idx_stray_windows_with_noise_within_half_window = None\n",
    "\n",
    "        output_data = pd.concat([normal_data, outlier_data])\n",
    "\n",
    "        window_start_ranges = np.arange(0, self.window_size+1 , 1)\n",
    "        window_ranges = np.vstack([window_start_ranges, window_start_ranges+self.window_size]).T\n",
    "        assert window_ranges.shape[0] == self.window_size+1, \"Should otuput one window with all normal data, then one window with one outlier, them with two, ...., finally a window with all outliers\"\n",
    "\n",
    "        iterator = iter(window_ranges)\n",
    "\n",
    "        def next_range():\n",
    "            return next(iterator)\n",
    "        \n",
    "        return {\n",
    "            'data': output_data,\n",
    "            'data_feature_names': input_data.data_feature_names, \n",
    "            'metadata_feature_names': input_data.metadata_feature_names,\n",
    "            'window_ranges': window_ranges,\n",
    "            '.next_range': next_range, \n",
    "            'noise_quantity': len(idx_stray_window_with_noise),\n",
    "            'noise_positions': idx_stray_window_with_noise.tolist(),\n",
    "            'last_idx_stray_window_with_noise': last_idx_stray_window_with_noise,\n",
    "            'idx_g1_equal_g2_size': idx_g1_equal_g2_size,\n",
    "            # 'injected_seq': [\"O\" if x else \"N\" for x in outlier_data.true_outlier],\n",
    "            'last_idx_stray_window_with_noise_within_half': idx_stray_windows_with_noise_within_half_window\n",
    "        }\n",
    "    \n",
    "\n",
    "def run_windows(S_data, stray_window_size, contaminazione):\n",
    "    ct = Context.load(ct_path)\n",
    "    n_wrong_outliers = int(floor(stray_window_size * contaminazione))\n",
    "    S_partitioner = StrayMixedNormalOutlierWindows(train_size=stray_window_size-1, test_size=1, window_shift=1, n_wrong_outliers=n_wrong_outliers,\n",
    "                                                        depends_on=S_data, ctx=ct)\n",
    "    # GET WINDOW NAMES\n",
    "    result_partitioner = S_partitioner.materialize()\n",
    "    window_ranges = result_partitioner.window_ranges\n",
    "\n",
    "    # GET PARTITIONS\n",
    "    S_train_plus_test_partitions = [NumericIndexBasedPartition(partitioner_function_name='.next_range', \n",
    "                                                            range_start=s, range_end=e,\n",
    "                                                            depends_on=S_partitioner, ctx=ct,\n",
    "                                                            name_alias=f\"WindowRange_{s}_{e}\") for s,e in window_ranges]\n",
    "\n",
    "    return {\n",
    "        'window_ranges': window_ranges,\n",
    "        'S_train_plus_test_partitions': S_train_plus_test_partitions,\n",
    "        'S_partitioner': S_partitioner,\n",
    "        'noise_quantity': result_partitioner.noise_quantity,\n",
    "        'noise_positions': result_partitioner.noise_positions,\n",
    "        'last_idx_stray_window_with_noise': result_partitioner.last_idx_stray_window_with_noise,\n",
    "        'idx_g1_equal_g2_size': result_partitioner.idx_g1_equal_g2_size,\n",
    "        'last_idx_stray_window_with_noise_within_half': result_partitioner.last_idx_stray_window_with_noise_within_half\n",
    "        }\n",
    "    \n",
    "    \n",
    "class AnnotateStrayWindowsInDF2(Step):\n",
    "    def run(self, partitioner_output, *original_data_enriched: list[pd.DataFrame]):\n",
    "        df_list = [x.output for x in original_data_enriched]\n",
    "        shared_index = partitioner_output.data.index.tolist()\n",
    "        table = np.zeros(( len(shared_index), len(df_list) ), dtype=bool)\n",
    "        window_size = df_list[0].shape[0]\n",
    "        for w_num, df in enumerate(df_list):\n",
    "            assert len(df['outlier']) == window_size, f\"Window {w_num} has {len(df['outlier'])} window size instead of {window_size}\"\n",
    "            assert df.index.tolist() == shared_index[w_num:w_num+window_size], f\"Window {w_num}: DF index is {df.index} while expected is {shared_index[w_num:w_num+window_size]}\"            \n",
    "            try:\n",
    "                table[w_num:w_num+window_size,w_num] = df['outlier']\n",
    "            except:\n",
    "                raise ValueError(\"Check window n. \", w_num)\n",
    "\n",
    "        stray_outliers = pd.DataFrame(table, columns=list(range(len(df_list))), index=shared_index)\n",
    "        return {\n",
    "            'output': stray_outliers,\n",
    "            'data_feature_names': np.arange(len(df_list)).tolist, \n",
    "            'metadata_feature_names': []\n",
    "        }\n",
    "    \n",
    "class CountWarnings(Step):\n",
    "    def __init__(self, k: int, idx_g1_equal_g2_size: int, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.k = k\n",
    "        self.idx_g1_equal_g2_size = idx_g1_equal_g2_size\n",
    "\n",
    "    def run(self, annotated_stray_window_in_df: ResultType[AnnotateStrayWindowsInDF2]):\n",
    "        stray_window_df = annotated_stray_window_in_df.output\n",
    "        n_runs = stray_window_df.shape[1]\n",
    "        self.w = n_runs - 1\n",
    "\n",
    "        n_warnings = 0\n",
    "        plw = 0 # position last warning\n",
    "        n_warnings_half = 0\n",
    "        plw_half = 0\n",
    "\n",
    "        for w_num in range(n_runs):\n",
    "            warnins_in_window = stray_window_df.iloc[w_num:w_num+self.w,w_num]\n",
    "            is_warning_last_seq = warnins_in_window.iloc[-1]\n",
    "\n",
    "            if is_warning_last_seq:\n",
    "                if w_num < self.idx_g1_equal_g2_size:\n",
    "                    n_warnings_half += 1\n",
    "                    plw_half = w_num\n",
    "                n_warnings += 1\n",
    "                plw = w_num\n",
    "            \n",
    "        return {\n",
    "            'n_warnings_global': n_warnings,\n",
    "            'plw_global': plw,\n",
    "            'n_warnings': n_warnings_half,\n",
    "            'plw': plw_half\n",
    "        }\n",
    "\n",
    "def run_stray4k(window_ranges, S_windows, S_partitioner, stray_k, idx_g1_equal_g2_size):\n",
    "    # STRAY\n",
    "    S_outlier_detection = [Stray.OutlierDetection(k=stray_k, depends_on=x, name_alias=f\"OutlierDetection{s}_{e}\") for x,(s,e) in zip(S_windows,window_ranges)]\n",
    "    S_train_plus_eval_partitions_enriched = [Stray.MapOutliersToOriginalData(depends_on=[s1,s2], \n",
    "                                                                            name_alias=f\"MapOutliersToOriginalData_{s}_{e}\") for s1,s2,(s,e) in zip(S_windows,S_outlier_detection,window_ranges)]\n",
    "    \n",
    "    # tabella che indica per ciascuna sequenza (riga) se Ã¨ un outlier o no nelle diverse run di stray (colonne). Una run per moving window\n",
    "    S_final_matrix = AnnotateStrayWindowsInDF2(depends_on=[S_partitioner, *S_train_plus_eval_partitions_enriched])\n",
    "    S_experiment_metrix = CountWarnings(stray_k, idx_g1_equal_g2_size, depends_on=S_final_matrix)\n",
    "    \n",
    "    # ##########  RUN\n",
    "    head = S_experiment_metrix\n",
    "    result = head.materialize()\n",
    "    return result\n",
    "\n",
    "def loop(window_sizes, ks, noise, repetitions):\n",
    "    global kwargs\n",
    "    tabular_results = []\n",
    "    combinazioni = [(w,k,n) for w in window_sizes for k in ks for n in noise if k < w and (n * w) == int(floor(n*w))] * repetitions   \n",
    "    print(len(combinazioni))\n",
    "    for window_size,k,n in tqdm(combinazioni):\n",
    "        kwargs = run_windows(S_data, window_size, n)\n",
    "\n",
    "        for k in ks:\n",
    "            if k >= window_size:\n",
    "                continue\n",
    "        \n",
    "            try:\n",
    "                result_unformatted = run_stray4k(\n",
    "                    window_ranges=kwargs['window_ranges'], \n",
    "                    S_windows=kwargs['S_train_plus_test_partitions'], \n",
    "                    S_partitioner=kwargs['S_partitioner'], \n",
    "                    stray_k=k, \n",
    "                    idx_g1_equal_g2_size=kwargs['idx_g1_equal_g2_size'])\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error while testing combination: {window_size} {k} {n}\")\n",
    "                raise e\n",
    "            \n",
    "            tabular_results.append([\n",
    "                window_size, k, n, kwargs['noise_quantity'], \n",
    "                result_unformatted['plw'], result_unformatted['n_warnings'], result_unformatted['plw_global'], result_unformatted['n_warnings_global'],\n",
    "                kwargs['last_idx_stray_window_with_noise_within_half'], kwargs['last_idx_stray_window_with_noise'], kwargs['idx_g1_equal_g2_size'], \n",
    "                kwargs['noise_positions']\n",
    "                ])             \n",
    "            \n",
    "\n",
    "    tabular_results = pd.DataFrame(tabular_results, columns=[\"N\", \"K\", \"Noise_perc\", \"Noise\", \n",
    "                                                             \"PLW\", \"N.Warnings\", \"PLW_global\", \"N.Warnings_global\", \n",
    "                                                             \"LNP\", \"LNP_global\", \"idx_G1>=G2\", \"NP\"])\n",
    "    return tabular_results\n",
    "\n",
    "kwargs = None\n",
    "tabular_results = loop(window_sizes=(5,10,50,100), ks=(1,3,5,10,15), noise=(0.0, 0.1, 0.2, 0.3), repetitions=10)     # this will repeat 10 times the experiment on window_size, noise  * len(ks), so 50 repetitions actually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabular_results.to_parquet(f\"{export_seq_id_path}/sensitivity_specificity10.parquet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
